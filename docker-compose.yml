# Tribal Memory â€” Docker Compose
#
# Usage:
#   With OpenAI:
#     OPENAI_API_KEY=sk-... docker compose up
#
#   With local Ollama:
#     docker compose --profile local up
#
# Data persists in ./tribal-memory-data/

services:
  tribal-memory:
    build: .
    ports:
      - "18790:18790"
    volumes:
      - ./tribal-memory-data:/data
      - ./config.yaml:/app/config.yaml:ro
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - TRIBAL_MEMORY_INSTANCE_ID=${TRIBAL_MEMORY_INSTANCE_ID:-docker}
    restart: unless-stopped

  # Optional: local Ollama for zero-cloud embeddings
  ollama:
    image: ollama/ollama:latest
    profiles: ["local"]
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped

volumes:
  ollama-data:
